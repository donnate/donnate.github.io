---
permalink: /projects/
title: "Research Projects"
excerpt: "Research Projects"
author_profile: true
---



<h1> Principled Inference, Statistics and Graph Neural Networks </h1>

<p align="justify">
As a direct adaptation of the deep neural network machinery to graph data, <b> Graph Neural Networks</b>  (GNNs – Hamilton et al. (2017); Kipf & Welling (2016)) <b>have recently gained considerable traction in the machine learning community.</b>  Heralded as the breakthrough for machine learning on graphs that would allow the same “AI renaissance” (Battaglia et al. (2018)) that standard neural networks have brought to Computer Vision and Natural Language Processing, GNNs have been suggested as  a  panacea  for  a  wide  number  of  tasks  across  disciplines,  including  molecular  design, traffic prediction, and machine learning for knowledge graphs or recommender systems.
</p>
<p style="color:grey;font-size:11px;" align="center">
<img src="http://donnate.github.io/images/debunking_gnns (1).png" />
<i>Graph Neural Network Block: Illustration of the aggregation (convolution) and transformation steps within each GNN block. GNNs blocks are typically concatenated to allow information to percolate from all parts of the network to each node.</i>
</p>

<p align="justify">
Of note is their promising application to biological networks (Li et al. (2021)) — a rich and rapidly expanding body of literature, largely due these networks’ versatility in describing systems of various interacting elements.
</p>



<p align="justify">
Yet, despite GNNs’ success on reference datasets in the academic community, both their properties and limitations remain ill-understood. This severely compromises their deployability to any practical or decision-making setting, particularly in a clinical or drug-development context — <b> settings in which the strong requirements for rigour, transparency and reliability clearly highlight the current limitations of Deep Learning methods for graph data.</b>
</p>

<p align="justify">
Motivated by the necessity to shift GNNs from a “black-box” model to an actionable ML method that can be explained, trusted and relied upon in practical settings, our group focuses on the analysis, improvement and applications of GNNs and graph algorithms at large, with a special focus on:
+ Uncertainty Quantification: 

</p>
__References__
+ William  L  Hamilton,  Rex  Ying,  and  Jure  Leskovec.   Inductive  representation  learning  on  large graphs.arXiv preprint arXiv:1706.02216, 2017.
+ Thomas N Kipf and Max Welling.   Semi-supervised classification with graph convolutional net-works.arXiv preprint arXiv:1609.02907, 2016.
+ Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,Mateusz  Malinowski,  Andrea  Tacchetti,  David  Raposo,  Adam  Santoro,  Ryan  Faulkner,  et  al.Relational inductive biases, deep learning, and graph networks.arXiv preprint arXiv:1806.01261, 2018
+ Michelle M Li, Kexin Huang, and Marinka Zitnik. Representation learning for networks in biologyand medicine:  Advancements, challenges, and opportunities.arXiv preprint arXiv:2104.04883,2021.


<h2> Defining graphs </h2>





<h2> COVID-19  </h2>
